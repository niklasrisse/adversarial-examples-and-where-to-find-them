{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum inter-class distances of all points of different dataset in different norms in table form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='paper')\n",
    "\n",
    "import provable_robustness_max_linear_regions.data as dt\n",
    "from utils import NumpyEncoder, normalize_per_feature_0_1, har, tinyimagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 26\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "# dictionary that maps color string to 'good looking' seaborn colors that are easily distinguishable\n",
    "colors = {\n",
    "    \"orange\": sns.xkcd_rgb[\"yellowish orange\"],\n",
    "    \"red\": sns.xkcd_rgb[\"pale red\"],\n",
    "    \"green\": sns.xkcd_rgb[\"medium green\"],\n",
    "    \"blue\": sns.xkcd_rgb[\"denim blue\"],\n",
    "    \"yellow\": sns.xkcd_rgb[\"amber\"],\n",
    "    \"purple\": sns.xkcd_rgb[\"dusty purple\"],\n",
    "    \"cyan\": sns.xkcd_rgb[\"cyan\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distances:\n",
    "Estimated runtime (if no file with data is present): 3 days\n",
    "\n",
    "Note: The dataset HAR is not included in this repository because of storage issues. You can download the dataset from https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones. After downloading, create a folder 'har' in the root folder of the repository and extract the dataset into the folder.\n",
    "\n",
    "Note: The dataset TINY-IMAGENET-200 is not included in this repository because of storage issues. You can download the dataset from https://tiny-imagenet.herokuapp.com/. After downloading, create a folder 'tiny-imagenet-200' in the root folder of the repository and extract the dataset into the folder.\n",
    "\n",
    "Without downloading the two datasets, the following code will not be executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(file_name):\n",
    "\n",
    "    if not os.path.exists(\"res/\" + file_name + \".json\"):\n",
    "        return None\n",
    "    else:\n",
    "        with open(\"res/\" + file_name + \".json\", 'r') as fp:\n",
    "            return json.load(fp)\n",
    "        \n",
    "def save_to_json(dictionary, file_name):\n",
    "        \n",
    "    if not os.path.exists(\"res\"):\n",
    "        os.makedirs(\"res\")\n",
    "\n",
    "    with open(\"res/\" + file_name + \".json\", 'w') as fp:\n",
    "        json.dump(dictionary, fp, cls = NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_n_points = {\"mnist\": 10000, \"fmnist\": 10000, \"cifar10\": 10000, \"gts\": 10000, \"tinyimagenet\": 98179, \"har\": 2947}\n",
    "\n",
    "minimum_distances = dict()\n",
    "\n",
    "for dataset in [\"mnist\", \"fmnist\", \"gts\", \"har\", \"tinyimagenet\", \"cifar10\"]:\n",
    "    \n",
    "    minimum_distances[dataset] = load_from_json(\"min_distances_dataset={}_n_points={}\".format(dataset, dataset_to_n_points[dataset]))\n",
    "\n",
    "    if not minimum_distances[dataset]:\n",
    "        \n",
    "        if dataset in [\"mnist\", \"fmnist\"]:\n",
    "            \n",
    "            _, x_test, _, y_test = dt.get_dataset(dataset)\n",
    "            sample_inputs = x_test[:dataset_to_n_points[dataset]]\n",
    "            sample_labels = y_test[:dataset_to_n_points[dataset]]\n",
    "            \n",
    "            sample_inputs = sample_inputs.reshape(sample_inputs.shape[0], 784)\n",
    "            \n",
    "        elif dataset in [\"gts\", \"cifar10\"]:\n",
    "        \n",
    "            _, x_test, _, y_test = dt.get_dataset(dataset)\n",
    "            sample_inputs = x_test[:dataset_to_n_points[dataset]]\n",
    "            sample_labels = y_test[:dataset_to_n_points[dataset]]\n",
    "            \n",
    "            sample_inputs = sample_inputs.reshape(sample_inputs.shape[0], 3072)\n",
    "            \n",
    "        elif dataset == \"har\":\n",
    "        \n",
    "            _, _, x_test, y_test, _ = har()\n",
    "            sample_inputs = x_test[:dataset_to_n_points[dataset]]\n",
    "            sample_labels = y_test[:dataset_to_n_points[dataset]]\n",
    "        \n",
    "        elif dataset == \"tinyimagenet\":\n",
    "\n",
    "            x_train, y_train = tinyimagenet()\n",
    "            sample_inputs = x_train[:dataset_to_n_points[dataset]]\n",
    "            sample_labels = y_train[:dataset_to_n_points[dataset]]\n",
    "            \n",
    "            sample_inputs = sample_inputs.reshape(sample_inputs.shape[0], 12288)\n",
    "\n",
    "        \n",
    "\n",
    "        minimum_distances[dataset] = {\"inner\": {\"inf\": [], \"2\": [], \"1\": []}, \"outer\": {\"inf\": [], \"2\": [], \"1\": []}}\n",
    "        scipy_norm_to_key = {\"chebyshev\": \"inf\", \"l2\": \"2\", \"l1\": \"1\"}\n",
    "\n",
    "        for norm in ['chebyshev', 'l2', 'l1']:\n",
    "\n",
    "            pairwise_distances = dist(sample_inputs, sample_inputs, norm)\n",
    "            np.fill_diagonal(pairwise_distances, np.inf)\n",
    "            \n",
    "            for i, sample_input in enumerate(sample_inputs):\n",
    "                row = pairwise_distances[i]\n",
    "                label = sample_labels[i].argmax()\n",
    "                inner_class_row = [x if sample_labels[j].argmax() == label else np.inf for j, x in enumerate(row)]\n",
    "                minimum_distances[dataset][\"inner\"][scipy_norm_to_key[norm]].append(np.min(inner_class_row))\n",
    "\n",
    "            minimum_distances[dataset][\"inner\"][scipy_norm_to_key[norm]] = np.sort(minimum_distances[dataset][\"inner\"][scipy_norm_to_key[norm]])\n",
    "            \n",
    "            for i, sample_input in enumerate(sample_inputs):\n",
    "                row = pairwise_distances[i]\n",
    "                label = sample_labels[i].argmax()\n",
    "                inner_class_row = [x if sample_labels[j].argmax() != label else np.inf for j, x in enumerate(row)]\n",
    "                minimum_distances[dataset][\"outer\"][scipy_norm_to_key[norm]].append(np.min(inner_class_row))\n",
    "\n",
    "            minimum_distances[dataset][\"outer\"][scipy_norm_to_key[norm]] = np.sort(minimum_distances[dataset][\"outer\"][scipy_norm_to_key[norm]])\n",
    "\n",
    "        save_to_json(minimum_distances[dataset], \"min_distances_dataset={}_n_points={}\".format(dataset, dataset_to_n_points[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset & Samples & Classes & Dimensionality & \\ell_\\infty & \\ell_2 & \\ell_1 & \\ell_\\infty & \\ell_2 & \\ell_1\n",
      "MNIST & 10000 & 10 & $28 \\times 28 \\times 1$ & 0.88 & 3.03 & 19.16 & 1.00 & 10.18 & 132.38\n",
      "TINY-IMG & 98179 & 200 & $64 \\times 64 \\times 3$ & 0.27 & 5.24 & 369.29 & 0.71 & 47.49 & 4184.37\n",
      "FMNIST & 10000 & 10 & $28 \\times 28 \\times 1$ & 0.36 & 2.00 & 24.87 & 1.00 & 10.70 & 194.29\n",
      "GTS & 10000 & 43 & $32 \\times 32 \\times 3$ & 0.07 & 0.90 & 31.46 & 0.62 & 19.54 & 833.22\n",
      "CIFAR10 & 10000 & 10 & $32 \\times 32 \\times 3$ & 0.27 & 3.61 & 130.77 & 0.70 & 18.57 & 831.44\n",
      "HAR & 2947 & 6 & $561$ & 0.26 & 1.26 & 12.95 & 0.87 & 4.29 & 73.19\n"
     ]
    }
   ],
   "source": [
    "dataset_to_name = {\"mnist\": \"MNIST\", \"fmnist\": \"FMNIST\", \"cifar10\": \"CIFAR10\", \"gts\": \"GTS\", \"tinyimagenet\": \"TINY-IMG\", \"har\": \"HAR\"}\n",
    "dataset_to_n_points = {\"mnist\": 10000, \"fmnist\": 10000, \"cifar10\": 10000, \"gts\": 10000, \"tinyimagenet\": 98179, \"har\": 2947}\n",
    "dataset_to_n_classes = {\"mnist\": 10, \"fmnist\": 10, \"cifar10\": 10, \"gts\": 43, \"tinyimagenet\": 200, \"har\": 6}\n",
    "dataset_to_dim = {\"mnist\": \"$28 \\\\times 28 \\\\times 1$\", \"fmnist\": \"$28 \\\\times 28 \\\\times 1$\", \"cifar10\": \"$32 \\\\times 32 \\\\times 3$\", \"gts\": \"$32 \\\\times 32 \\\\times 3$\", \"tinyimagenet\": \"$64 \\\\times 64 \\\\times 3$\", \"har\": \"$561$\"}\n",
    "\n",
    "print(\"Dataset & Samples & Classes & Dimensionality & \\ell_\\infty & \\ell_2 & \\ell_1 & \\ell_\\infty & \\ell_2 & \\ell_1\")\n",
    "for dataset in [\"mnist\", \"tinyimagenet\", \"fmnist\", \"gts\", \"cifar10\", \"har\"]:\n",
    "    for norm in [\"inf\", \"2\", \"1\"]:\n",
    "        minimum_distances[dataset][\"outer\"][norm] = [value for value in minimum_distances[dataset][\"outer\"][norm] if value >= 0.0001]\n",
    "    print(\"{} & {} & {} & {} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f} & {:.2f}\".format(dataset_to_name[dataset], dataset_to_n_points[dataset], dataset_to_n_classes[dataset], dataset_to_dim[dataset], np.min(minimum_distances[dataset][\"outer\"][\"inf\"]), np.min(minimum_distances[dataset][\"outer\"][\"2\"]), np.min(minimum_distances[dataset][\"outer\"][\"1\"]), np.max(minimum_distances[dataset][\"outer\"][\"inf\"]), np.max(minimum_distances[dataset][\"outer\"][\"2\"]), np.max(minimum_distances[dataset][\"outer\"][\"1\"]))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
